---
title: "Parallelizing R code"
author: "Pablo Rodríguez-Sánchez"
date: "4/28/2020"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Parallelizing R code
This tutorial is strongly based on [this one](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html). 

The main difference is merely pedagogical. I use a more compact approach and reuse the same problem again and again across all the examples.

## Why parallelizing?

Modern laptops typically have 4 or 8 cores. Each of them can be loosely thought of as an independent mini-computer, capable of doing tasks independently of the other cores. The number of cores in your computer can be retrieved from `R` with the command:

```{r number-cores}
numCores <- parallel::detectCores() # Requires library(parallel)
print(numCores)
```

When a program runs serial (as they usually run by default), only one core is recruited for performing the program's task. Recruiting more than one core for a given task is known as parallelization. The desired result of parallelizing a task is a reduction in its execution time. An analogy could be to build a wall by laying bricks alone, one by one (serial) as opposed to building it with the help of three friends (parallel with 4 "cores").

It may be tempting to think that doubling the amount of cores devoted to a given task will divide by two its execution time. But, as we'll see, this is far from true. Parallelization rarely behaves linearly. A reason for this is that the code, the input, and often also the data, has to be copied to each of the cores. The output of each core also has to be put together, and this also consumes time. Following the analogy of the 4 brick-layers, it is clear that building a wall with friends requires a bit of planning and coordination before starting laying the bricks (at least if we want the wall to be one wall, and not four disconnected pieces). Intuition also tells us that, while perhaps asking for the help of 4 friends may be a good idea, calling 256 friends could be an organizational nightmare. For a more formal approach to these ideas, check [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law).

Even worse, some problems are not parallelizable at all due to its very nature. For instance, what if instead of a brick wall we want to make a brick stack, with only one brick per level? There is no point of laying the 3rd brick if the 1st and 2nd are not there already! We'll see one more example later. 

## Generate the task
A good example task for showing the advantages of parallelization would be one that:

1. Is simple.
2. Takes an appreciable amount of CPU time to be completed.

The problem of deciding if a large integer is prime or not fulfills both charachteristics.

It is a good idea to split a task into a function (describing WHAT to do) ...

```{r define-fun}
fun <- function(x) {
  numbers::isPrime(x) # Requires the package numbers
}
```

... and an input (describing to WHOM).

```{r define-input}
# Generate some large integers
N <- 1000 # Number of integers
min <- 100000 # Lower bound
max <- 10000000000 # Upper bound

inputs <- sample(min:max, N) # List of N random large integers (between 1e5 and 1e10)
```

These two chunks of code define the _homeworks_ we want to assign to our computer. Particularly, we want our computer to decide which integers from a list of inputs are prime, and store the results. We will do this in six different ways, two of them parallel. Later, we'll compare their performance.

Note: the astute reader may have noticed that our function `fun` is a mere renaming of the function `isPrime` contained in the package `numbers`. The reason for that is merely pedagogical: we want the reader to:

1. Notice that the structure input -> function -> results is indeed VERY general
2. Be able to try his/her own pieces of slow code inside the body of `fun`.

### A note about the examples

Please note that, in the following examples, the structure below just times whatever `<code>` is between the braces, and has nothing to do with the parallelization. 

```
system.time({

  <code>
  
}) -> execution_time
```

## Possibility 0: run serial

In this section we'll see three different ways of solving our problem running serial. That is, without using parallelization. The user is likely to be familiar with at least some of them.

### 0.1 Run serial with a loop

```{r serial-loop}
system.time({
  
  results <- rep(NA, N) # Initialize results vector  
  for (i in 1:N) { # Loop one by one along the input elements
    results[i] <- fun(inputs[i])
  }
  
}) -> time_serial_loop
```

### 0.2 Run serial with `foreach`
This command avoids us explicit typing of the index (compare with the `for` loop in the previous example). The output of `foreach` is, by default, a list. We used the parameter `.combine = "c"` (concatenate) to force it into a vector.

```{r serial-foreach}
# Load the required libraries
library(foreach)

system.time({
  
  foreach (val = inputs, .combine = "c") %do% { # Loop one by one using foreach
    fun(val)
  } -> results
  
}) -> time_serial_foreach
```

### 0.3 Run serial with `lapply`

```{r serial-lapply}
system.time({
  
  results <- lapply(inputs, fun) # Hide your loop with an lapply function

}) -> time_serial_lapply
```


## Do it parallel!

In this section we'll finally use parallel programming to solve our problem.

### Possibility 1: run parallel with `mclapply` (Linux only)

`mclapply` is part of the `parallel` library. Rule of thumb: use it instead of `lapply`. If the code is parallelizable, it will do the magic:

```{r mclapply}
# Load the required libraries
library(parallel)

# Run parallel
system.time({
  
  results <- mclapply(inputs, fun, mc.cores = numCores) # Note that the number of cores is required by mclapply

}) -> time_mclapply
```

Unfortunately, this approach only works on Linux. If you are working on a Windows machine, it will perform as a non-parallel `lapply`.

### Possibility 2: run parallel with `doParallel` + `foreach`

Rule of thumb: `doParallel` transforms a `foreach` loop into a parallel process. Of course, the underlying process should be parallelizable. 

```{r doParallel}
# Load the required libraries
library(iterators)
library(doParallel)

# Initialize
registerDoParallel(numCores)

# Loop
system.time({
  
  foreach(val = inputs, .combine = "c") %dopar% {
    fun(val)
  } -> results
  
}) -> time_doParallel
```

## Possibility 3

```{r serial-vector}
system.time({

  results <- fun(inputs)
  
}) -> time_vector
```

## Compare results

By plotting the execution times, we notice that the parallel methods performed significantly better than the serial ones:

```{r echo=FALSE, message=FALSE, warning=FALSE}
i <- 3
times_df <- data.frame(
  case = c("0.1 Loop", "0.2 foreach", "0.3 lapply", "1 mclapply", "2 doParallel", "3 Vector"),
  time = c(time_serial_loop[i], time_serial_foreach[i], time_serial_lapply[i], time_mclapply[i], time_doParallel[i], time_vector[i]),
  mode = c("Serial", "Serial", "Serial", "Parallel", "Parallel", "Serial vectorized"),
  stringsAsFactors = TRUE
)

library(ggplot2)
ggplot(times_df, aes(x = case, y = time, fill = mode)) + geom_bar(stat = "identity") + labs(x = "", y = "time (s)", fill = "")
```

## Trying to parallelize non-parallelizable code

The loop structure of possibility 2 increases the chances of accidentally trying to parallelize code that is not parallelizable. In this subsection we'll use the iterator below as an example:

$$
x_{n} = x_{n-1} + 1
$$

It is easy to see that, when initialized with $x_{0} = 0$, this iterator yields $\{0, 1, 2, 3, 4, ...\}$. Note that in order to obtain $x_n$, the value of the previous x (i.e.: $x_{n-1}$) is required. This operation is thus intrinsically serial, and cannot be parallelized.

```{r}
fun <- function(x) {x + 1}

N <- 6
inputs <- rep(NA, N) # Initialize as (0, NA, NA, ..., NA)
inputs[1] <- 0 
```

Using a serial loop everything works fine:

```{r, results='hide'}
foreach(i = 2:N) %do% {
  inputs[i] <- fun(inputs[i-1])
}
```

```{r, echo=FALSE}
print(inputs) # This hidden chunk just creates a nicer output
```

Trying to force parallelization doesn't work, and also doesn't throw an error!

```{r, results='hide'}
inputs <- rep(NA, N) # Initialize again
inputs[1] <- 0

foreach(i = 2:N) %dopar% {
  inputs[i] <- fun(inputs[i-1])
}
```

```{r, echo=FALSE}
print(inputs) # This hidden chunk just creates a nicer output
```
